{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "yolov2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aAENSonaQ1p",
        "outputId": "2c0fc219-2095-4da9-c484-816bf87d774c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_eD5IyJaicv",
        "outputId": "349ac91e-536e-4d57-a0ff-09933e1eb511"
      },
      "source": [
        "%cd '/content/drive/My Drive/YoloV2/darkflow-master/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/YoloV2/darkflow-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbdhff9tbQEo",
        "outputId": "d9f90b07-1b83-448f-98a1-2ec479c5ea56"
      },
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/70/45d3b9fab768215a2055c7819d39547a4b0b7401b4583094068741aff99b/tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.32.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (54.1.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGpi8tjmcVE6",
        "outputId": "3e69624e-3137-42ba-e725-0c27a04d3707"
      },
      "source": [
        "!pip install codecov"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting codecov\n",
            "  Downloading https://files.pythonhosted.org/packages/93/9f/bbea5b6231308458963cb5c067bc5643da9949689702fa5a382714b59699/codecov-2.1.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.7/dist-packages (from codecov) (3.7.1)\n",
            "Requirement already satisfied: requests>=2.7.9 in /usr/local/lib/python3.7/dist-packages (from codecov) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.9->codecov) (3.0.4)\n",
            "Installing collected packages: codecov\n",
            "Successfully installed codecov-2.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp1EtHGpggOg",
        "outputId": "f10fec49-0845-4b90-f9dc-1f9024f6355e"
      },
      "source": [
        "!pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/drive/My Drive/YoloV2/darkflow-master\n",
            "Building wheels for collected packages: darkflow\n",
            "  Building wheel for darkflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for darkflow: filename=darkflow-1.0.0-cp37-cp37m-linux_x86_64.whl size=813103 sha256=2588e3d2a44503dd60659910a6bdd8d073ee86d775c797a557a7ecdbbfcc64b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/1f/90/323b8575dbfc9db0c3115c4ffc77dae58dc703e9026665591f\n",
            "Successfully built darkflow\n",
            "Installing collected packages: darkflow\n",
            "Successfully installed darkflow-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGwzWvKJfQS5",
        "outputId": "6805ef9a-edb7-4fe8-af32-82bae00fc108"
      },
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-3.7/darkflow/cython_utils/nms.cpython-37m-x86_64-linux-gnu.so -> darkflow/cython_utils\n",
            "copying build/lib.linux-x86_64-3.7/darkflow/cython_utils/cy_yolo2_findboxes.cpython-37m-x86_64-linux-gnu.so -> darkflow/cython_utils\n",
            "copying build/lib.linux-x86_64-3.7/darkflow/cython_utils/cy_yolo_findboxes.cpython-37m-x86_64-linux-gnu.so -> darkflow/cython_utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eASSfEmtaz2v",
        "outputId": "67ea9c70-c997-4e76-a7f7-1f83b251b8e6"
      },
      "source": [
        "!python flow --model cfg/tiny-yolo-voc-5c.cfg --load bin/yolov2-tiny.weights --train --annotation bin/annotation --dataset bin/data_image --gpu 1.0 --epoch 1500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\n",
            "Parsing cfg/tiny-yolo-voc-5c.cfg\n",
            "Loading None ...\n",
            "Finished in 0.0001246929168701172s\n",
            "\n",
            "Building net ...\n",
            "Source | Train? | Layer description                | Output size\n",
            "-------+--------+----------------------------------+---------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "       |        | input                            | (?, 416, 416, 3)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 16)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 16)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 32)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 32)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 64)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 64)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 128)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 128)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 256)\n",
            " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 256)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 512)\n",
            " Load  |  Yep!  | maxp 2x2p0_1                     | (?, 13, 13, 512)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
            " Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)\n",
            " Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 50)\n",
            "-------+--------+----------------------------------+---------------\n",
            "GPU mode with 1.0 usage\n",
            "cfg/tiny-yolo-voc-5c.cfg loss hyper-parameters:\n",
            "\tH       = 13\n",
            "\tW       = 13\n",
            "\tbox     = 5\n",
            "\tclasses = 5\n",
            "\tscales  = [1.0, 5.0, 1.0, 1.0]\n",
            "WARNING:tensorflow:From /content/drive/My Drive/YoloV2/darkflow-master/darkflow/net/yolov2/train.py:87: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Building cfg/tiny-yolo-voc-5c.cfg loss\n",
            "Building cfg/tiny-yolo-voc-5c.cfg train op\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2021-03-24 11:24:40.881164: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-03-24 11:24:40.921266: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2021-03-24 11:24:40.924231: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x557d46aed080 executing computations on platform Host. Devices:\n",
            "2021-03-24 11:24:40.924269: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "Loading from ./ckpt/tiny-yolo-voc-5c-6000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Finished in 6.098010778427124s\n",
            "\n",
            "Enter training ...\n",
            "\n",
            "cfg/tiny-yolo-voc-5c.cfg parsing bin/annotation\n",
            "Parsing for ['Carro', 'Caminhao', 'Onibus', 'Moto', 'Bicicleta'] \n",
            "[====================>]100%  51.xml\n",
            "Statistics:\n",
            "Carro: 86\n",
            "Caminhao: 66\n",
            "Onibus: 5\n",
            "Moto: 39\n",
            "Bicicleta: 13\n",
            "Dataset size: 209\n",
            "Dataset of 209 instance(s)\n",
            "Training statistics: \n",
            "\tLearning rate : 1e-05\n",
            "\tBatch size    : 16\n",
            "\tEpoch number  : 1500\n",
            "\tBackup every  : 2000\n",
            "step 6001 - loss 1.6692895889282227 - moving ave loss 1.6692895889282227\n",
            "step 6002 - loss 1.8824076652526855 - moving ave loss 1.690601396560669\n",
            "step 6003 - loss 1.5101518630981445 - moving ave loss 1.6725564432144164\n",
            "step 6004 - loss 1.7086119651794434 - moving ave loss 1.676161995410919\n",
            "step 6005 - loss 1.4187054634094238 - moving ave loss 1.6504163422107696\n",
            "step 6006 - loss 1.5591559410095215 - moving ave loss 1.6412903020906449\n",
            "step 6007 - loss 1.613595724105835 - moving ave loss 1.638520844292164\n",
            "step 6008 - loss 1.7023760080337524 - moving ave loss 1.6449063606663228\n",
            "step 6009 - loss 1.6971030235290527 - moving ave loss 1.650126026952596\n",
            "step 6010 - loss 1.5588116645812988 - moving ave loss 1.6409945907154662\n",
            "step 6011 - loss 1.5309853553771973 - moving ave loss 1.6299936671816393\n",
            "step 6012 - loss 1.8197180032730103 - moving ave loss 1.6489661007907763\n",
            "step 6013 - loss 1.6049740314483643 - moving ave loss 1.6445668938565352\n",
            "Finish 1 epoch(es)\n",
            "step 6014 - loss 1.7122920751571655 - moving ave loss 1.6513394119865983\n",
            "step 6015 - loss 1.510798692703247 - moving ave loss 1.6372853400582632\n",
            "step 6016 - loss 1.7406623363494873 - moving ave loss 1.6476230396873857\n",
            "step 6017 - loss 1.7717864513397217 - moving ave loss 1.6600393808526195\n",
            "step 6018 - loss 1.6756713390350342 - moving ave loss 1.661602576670861\n",
            "step 6019 - loss 1.9434744119644165 - moving ave loss 1.6897897602002168\n",
            "step 6020 - loss 1.5782372951507568 - moving ave loss 1.678634513695271\n",
            "step 6021 - loss 1.6502976417541504 - moving ave loss 1.675800826501159\n",
            "step 6022 - loss 1.2957568168640137 - moving ave loss 1.6377964255374444\n",
            "step 6023 - loss 1.6975613832473755 - moving ave loss 1.6437729213084378\n",
            "step 6024 - loss 1.5726771354675293 - moving ave loss 1.636663342724347\n",
            "step 6025 - loss 1.658250093460083 - moving ave loss 1.6388220177979207\n",
            "step 6026 - loss 1.6470791101455688 - moving ave loss 1.6396477270326855\n",
            "Finish 2 epoch(es)\n",
            "step 6027 - loss 1.5554752349853516 - moving ave loss 1.6312304778279523\n",
            "step 6028 - loss 1.627453088760376 - moving ave loss 1.6308527389211946\n",
            "step 6029 - loss 1.5917092561721802 - moving ave loss 1.6269383906462933\n",
            "step 6030 - loss 1.7059154510498047 - moving ave loss 1.6348360966866444\n",
            "step 6031 - loss 1.585405945777893 - moving ave loss 1.6298930815957693\n",
            "step 6032 - loss 1.82167649269104 - moving ave loss 1.6490714227052965\n",
            "step 6033 - loss 1.8146016597747803 - moving ave loss 1.6656244464122447\n",
            "step 6034 - loss 1.6322126388549805 - moving ave loss 1.6622832656565185\n",
            "step 6035 - loss 1.703878402709961 - moving ave loss 1.6664427793618628\n",
            "step 6036 - loss 1.63999342918396 - moving ave loss 1.6637978443440726\n",
            "step 6037 - loss 1.8275017738342285 - moving ave loss 1.6801682372930882\n",
            "step 6038 - loss 1.838135004043579 - moving ave loss 1.6959649139681374\n",
            "step 6039 - loss 1.696660041809082 - moving ave loss 1.6960344267522318\n",
            "Finish 3 epoch(es)\n",
            "step 6040 - loss 1.4248591661453247 - moving ave loss 1.6689169006915412\n",
            "step 6041 - loss 1.9274166822433472 - moving ave loss 1.6947668788467218\n",
            "step 6042 - loss 1.3250062465667725 - moving ave loss 1.657790815618727\n",
            "step 6043 - loss 1.6684490442276 - moving ave loss 1.6588566384796144\n",
            "step 6044 - loss 1.6290396451950073 - moving ave loss 1.6558749391511538\n",
            "step 6045 - loss 1.6530414819717407 - moving ave loss 1.6555915934332126\n",
            "step 6046 - loss 1.5821824073791504 - moving ave loss 1.6482506748278065\n",
            "step 6047 - loss 1.6764298677444458 - moving ave loss 1.6510685941194705\n",
            "step 6048 - loss 1.6470317840576172 - moving ave loss 1.6506649131132853\n",
            "step 6049 - loss 1.5635876655578613 - moving ave loss 1.641957188357743\n",
            "step 6050 - loss 1.687471866607666 - moving ave loss 1.6465086561827353\n",
            "step 6051 - loss 1.6397442817687988 - moving ave loss 1.645832218741342\n",
            "step 6052 - loss 1.7309364080429077 - moving ave loss 1.6543426376714985\n",
            "Finish 4 epoch(es)\n",
            "step 6053 - loss 1.5621758699417114 - moving ave loss 1.6451259608985198\n",
            "step 6054 - loss 1.7046478986740112 - moving ave loss 1.651078154676069\n",
            "step 6055 - loss 1.714738130569458 - moving ave loss 1.657444152265408\n",
            "step 6056 - loss 1.7660577297210693 - moving ave loss 1.668305510010974\n",
            "step 6057 - loss 1.6029536724090576 - moving ave loss 1.6617703262507824\n",
            "step 6058 - loss 1.7072343826293945 - moving ave loss 1.6663167318886436\n",
            "step 6059 - loss 1.6759535074234009 - moving ave loss 1.6672804094421192\n",
            "step 6060 - loss 1.6560442447662354 - moving ave loss 1.666156792974531\n",
            "step 6061 - loss 1.7527414560317993 - moving ave loss 1.6748152592802577\n",
            "step 6062 - loss 1.4866478443145752 - moving ave loss 1.6559985177836893\n",
            "step 6063 - loss 1.5356606245040894 - moving ave loss 1.6439647284557293\n",
            "step 6064 - loss 1.6735676527023315 - moving ave loss 1.6469250208803896\n",
            "step 6065 - loss 1.6539888381958008 - moving ave loss 1.6476314026119308\n",
            "Finish 5 epoch(es)\n",
            "step 6066 - loss 1.5067975521087646 - moving ave loss 1.6335480175616142\n",
            "step 6067 - loss 1.396559715270996 - moving ave loss 1.6098491873325522\n",
            "step 6068 - loss 1.6375092267990112 - moving ave loss 1.6126151912791982\n",
            "step 6069 - loss 1.705660343170166 - moving ave loss 1.6219197064682949\n",
            "step 6070 - loss 1.5992475748062134 - moving ave loss 1.6196524933020866\n",
            "step 6071 - loss 1.5673036575317383 - moving ave loss 1.6144176097250518\n",
            "step 6072 - loss 1.5466915369033813 - moving ave loss 1.6076450024428848\n",
            "step 6073 - loss 1.8401272296905518 - moving ave loss 1.6308932251676516\n",
            "step 6074 - loss 1.705897569656372 - moving ave loss 1.6383936596165236\n",
            "step 6075 - loss 1.4943305253982544 - moving ave loss 1.6239873461946968\n",
            "step 6076 - loss 1.311082124710083 - moving ave loss 1.5926968240462354\n",
            "step 6077 - loss 1.5956337451934814 - moving ave loss 1.5929905161609599\n",
            "step 6078 - loss 1.6941324472427368 - moving ave loss 1.6031047092691377\n",
            "Finish 6 epoch(es)\n",
            "step 6079 - loss 1.857696533203125 - moving ave loss 1.6285638916625367\n",
            "step 6080 - loss 1.5901644229888916 - moving ave loss 1.6247239447951722\n",
            "step 6081 - loss 1.9176082611083984 - moving ave loss 1.654012376426495\n",
            "step 6082 - loss 1.4542791843414307 - moving ave loss 1.6340390572179886\n",
            "step 6083 - loss 1.5468236207962036 - moving ave loss 1.6253175135758102\n",
            "step 6084 - loss 1.6239991188049316 - moving ave loss 1.6251856740987223\n",
            "step 6085 - loss 1.4613114595413208 - moving ave loss 1.6087982526429823\n",
            "step 6086 - loss 1.5281274318695068 - moving ave loss 1.6007311705656346\n",
            "step 6087 - loss 1.3344066143035889 - moving ave loss 1.57409871493943\n",
            "step 6088 - loss 1.6433014869689941 - moving ave loss 1.5810189921423867\n",
            "step 6089 - loss 1.5145186185836792 - moving ave loss 1.574368954786516\n",
            "step 6090 - loss 1.3393402099609375 - moving ave loss 1.5508660803039582\n",
            "step 6091 - loss 1.377528429031372 - moving ave loss 1.5335323151766995\n",
            "Finish 7 epoch(es)\n",
            "step 6092 - loss 1.838366150856018 - moving ave loss 1.5640156987446314\n",
            "step 6093 - loss 1.5444486141204834 - moving ave loss 1.5620589902822166\n",
            "step 6094 - loss 1.5891623497009277 - moving ave loss 1.5647693262240878\n",
            "step 6095 - loss 1.5724732875823975 - moving ave loss 1.5655397223599188\n",
            "step 6096 - loss 1.647345781326294 - moving ave loss 1.5737203282565564\n",
            "step 6097 - loss 1.391856074333191 - moving ave loss 1.5555339028642199\n",
            "step 6098 - loss 1.7606537342071533 - moving ave loss 1.5760458859985134\n",
            "step 6099 - loss 1.9077054262161255 - moving ave loss 1.6092118400202746\n",
            "step 6100 - loss 1.6970031261444092 - moving ave loss 1.617990968632688\n",
            "step 6101 - loss 1.809551477432251 - moving ave loss 1.6371470195126445\n",
            "step 6102 - loss 1.576741099357605 - moving ave loss 1.6311064274971407\n",
            "step 6103 - loss 1.5773346424102783 - moving ave loss 1.6257292489884545\n",
            "step 6104 - loss 1.490911841392517 - moving ave loss 1.6122475082288608\n",
            "Finish 8 epoch(es)\n",
            "step 6105 - loss 1.5567686557769775 - moving ave loss 1.6066996229836725\n",
            "step 6106 - loss 1.613560438156128 - moving ave loss 1.6073857045009181\n",
            "step 6107 - loss 1.46262526512146 - moving ave loss 1.5929096605629722\n",
            "step 6108 - loss 1.5496392250061035 - moving ave loss 1.5885826170072854\n",
            "step 6109 - loss 1.6323509216308594 - moving ave loss 1.5929594474696427\n",
            "step 6110 - loss 1.411226749420166 - moving ave loss 1.5747861776646952\n",
            "step 6111 - loss 1.6435900926589966 - moving ave loss 1.5816665691641254\n",
            "step 6112 - loss 1.6324224472045898 - moving ave loss 1.586742156968172\n",
            "step 6113 - loss 1.3889192342758179 - moving ave loss 1.5669598646989367\n",
            "step 6114 - loss 1.3945508003234863 - moving ave loss 1.5497189582613917\n",
            "step 6115 - loss 1.7919420003890991 - moving ave loss 1.5739412624741624\n",
            "step 6116 - loss 1.5134129524230957 - moving ave loss 1.567888431469056\n",
            "step 6117 - loss 1.4025707244873047 - moving ave loss 1.5513566607708809\n",
            "Finish 9 epoch(es)\n",
            "step 6118 - loss 1.4441180229187012 - moving ave loss 1.540632796985663\n",
            "step 6119 - loss 1.6223390102386475 - moving ave loss 1.5488034183109614\n",
            "step 6120 - loss 1.709836721420288 - moving ave loss 1.5649067486218942\n",
            "step 6121 - loss 1.5081958770751953 - moving ave loss 1.5592356614672243\n",
            "step 6122 - loss 1.4162700176239014 - moving ave loss 1.544939097082892\n",
            "step 6123 - loss 1.3272420167922974 - moving ave loss 1.5231693890538327\n",
            "step 6124 - loss 1.6352195739746094 - moving ave loss 1.5343744075459103\n",
            "step 6125 - loss 1.6082065105438232 - moving ave loss 1.5417576178457015\n",
            "Checkpoint at step 6125\n",
            "step 6126 - loss 1.7131474018096924 - moving ave loss 1.5588965962421009\n",
            "step 6127 - loss 1.3882474899291992 - moving ave loss 1.5418316856108107\n",
            "step 6128 - loss 1.593372106552124 - moving ave loss 1.5469857277049421\n",
            "step 6129 - loss 1.3898859024047852 - moving ave loss 1.5312757451749266\n",
            "step 6130 - loss 1.6046946048736572 - moving ave loss 1.5386176311447997\n",
            "Finish 10 epoch(es)\n",
            "step 6131 - loss 1.231969952583313 - moving ave loss 1.5079528632886512\n",
            "step 6132 - loss 1.5839332342147827 - moving ave loss 1.5155509003812644\n",
            "step 6133 - loss 1.426259994506836 - moving ave loss 1.5066218097938215\n",
            "step 6134 - loss 1.4517666101455688 - moving ave loss 1.5011362898289962\n",
            "step 6135 - loss 1.7731136083602905 - moving ave loss 1.5283340216821257\n",
            "step 6136 - loss 1.7721586227416992 - moving ave loss 1.5527164817880832\n",
            "step 6137 - loss 1.6649987697601318 - moving ave loss 1.5639447105852882\n",
            "step 6138 - loss 1.4342899322509766 - moving ave loss 1.550979232751857\n",
            "step 6139 - loss 1.6190779209136963 - moving ave loss 1.557789101568041\n",
            "step 6140 - loss 1.4235080480575562 - moving ave loss 1.5443609962169924\n",
            "step 6141 - loss 1.3630188703536987 - moving ave loss 1.526226783630663\n",
            "step 6142 - loss 1.5811223983764648 - moving ave loss 1.5317163451052433\n",
            "step 6143 - loss 1.6080448627471924 - moving ave loss 1.5393491968694384\n",
            "Finish 11 epoch(es)\n",
            "step 6144 - loss 1.592287302017212 - moving ave loss 1.544643007384216\n",
            "step 6145 - loss 1.452829360961914 - moving ave loss 1.5354616427419856\n",
            "step 6146 - loss 1.3549760580062866 - moving ave loss 1.517413084268416\n",
            "step 6147 - loss 1.266798496246338 - moving ave loss 1.4923516254662081\n",
            "step 6148 - loss 1.4592562913894653 - moving ave loss 1.4890420920585339\n",
            "step 6149 - loss 1.5230638980865479 - moving ave loss 1.4924442726613352\n",
            "step 6150 - loss 1.4280452728271484 - moving ave loss 1.4860043726779164\n",
            "step 6151 - loss 1.5242106914520264 - moving ave loss 1.4898250045553274\n",
            "step 6152 - loss 1.5991811752319336 - moving ave loss 1.500760621622988\n",
            "step 6153 - loss 1.3686585426330566 - moving ave loss 1.487550413723995\n",
            "step 6154 - loss 1.6373335123062134 - moving ave loss 1.502528723582217\n",
            "step 6155 - loss 1.590270757675171 - moving ave loss 1.5113029269915124\n",
            "step 6156 - loss 1.5398129224777222 - moving ave loss 1.5141539265401334\n",
            "Finish 12 epoch(es)\n",
            "step 6157 - loss 1.7771967649459839 - moving ave loss 1.5404582103807183\n",
            "step 6158 - loss 1.2194602489471436 - moving ave loss 1.508358414237361\n",
            "step 6159 - loss 1.321427822113037 - moving ave loss 1.4896653550249286\n",
            "step 6160 - loss 1.7217905521392822 - moving ave loss 1.512877874736364\n",
            "step 6161 - loss 1.4681906700134277 - moving ave loss 1.5084091542640703\n",
            "step 6162 - loss 1.5917201042175293 - moving ave loss 1.5167402492594162\n",
            "step 6163 - loss 1.5070455074310303 - moving ave loss 1.5157707750765776\n",
            "step 6164 - loss 1.2720141410827637 - moving ave loss 1.4913951116771964\n",
            "step 6165 - loss 1.5173194408416748 - moving ave loss 1.4939875445936444\n",
            "step 6166 - loss 1.5530412197113037 - moving ave loss 1.4998929121054103\n",
            "step 6167 - loss 1.6205456256866455 - moving ave loss 1.511958183463534\n",
            "step 6168 - loss 1.2625808715820312 - moving ave loss 1.4870204522753838\n",
            "step 6169 - loss 1.4328432083129883 - moving ave loss 1.4816027278791444\n",
            "Finish 13 epoch(es)\n",
            "step 6170 - loss 1.7381110191345215 - moving ave loss 1.5072535570046823\n",
            "step 6171 - loss 1.6660138368606567 - moving ave loss 1.5231295849902797\n",
            "step 6172 - loss 1.7167658805847168 - moving ave loss 1.5424932145497234\n",
            "step 6173 - loss 1.6305097341537476 - moving ave loss 1.5512948665101258\n",
            "step 6174 - loss 1.6382060050964355 - moving ave loss 1.559985980368757\n",
            "step 6175 - loss 1.4405171871185303 - moving ave loss 1.5480391010437344\n",
            "step 6176 - loss 1.1594990491867065 - moving ave loss 1.5091850958580317\n",
            "step 6177 - loss 1.5535597801208496 - moving ave loss 1.5136225642843135\n",
            "step 6178 - loss 1.5584564208984375 - moving ave loss 1.518105949945726\n",
            "step 6179 - loss 1.6399173736572266 - moving ave loss 1.530287092316876\n",
            "step 6180 - loss 1.3984293937683105 - moving ave loss 1.5171013224620196\n",
            "step 6181 - loss 1.6175901889801025 - moving ave loss 1.5271502091138278\n",
            "step 6182 - loss 1.2576942443847656 - moving ave loss 1.5002046126409216\n",
            "Finish 14 epoch(es)\n",
            "step 6183 - loss 1.4975694417953491 - moving ave loss 1.4999410955563646\n",
            "step 6184 - loss 1.5485763549804688 - moving ave loss 1.504804621498775\n",
            "step 6185 - loss 1.6438379287719727 - moving ave loss 1.518707952226095\n",
            "step 6186 - loss 1.1996357440948486 - moving ave loss 1.4868007314129703\n",
            "step 6187 - loss 1.5152344703674316 - moving ave loss 1.4896441053084166\n",
            "step 6188 - loss 1.5685266256332397 - moving ave loss 1.497532357340899\n",
            "step 6189 - loss 1.57804274559021 - moving ave loss 1.50558339616583\n",
            "step 6190 - loss 1.5980517864227295 - moving ave loss 1.51483023519152\n",
            "step 6191 - loss 1.208749771118164 - moving ave loss 1.4842221887841844\n",
            "step 6192 - loss 1.58243727684021 - moving ave loss 1.494043697589787\n",
            "step 6193 - loss 1.5524808168411255 - moving ave loss 1.499887409514921\n",
            "step 6194 - loss 1.3930227756500244 - moving ave loss 1.4892009461284312\n",
            "step 6195 - loss 1.7415210008621216 - moving ave loss 1.5144329516018002\n",
            "Finish 15 epoch(es)\n",
            "step 6196 - loss 1.474029779434204 - moving ave loss 1.5103926343850407\n",
            "step 6197 - loss 1.501039743423462 - moving ave loss 1.5094573452888829\n",
            "step 6198 - loss 1.7365775108337402 - moving ave loss 1.5321693618433687\n",
            "step 6199 - loss 1.558976650238037 - moving ave loss 1.5348500906828357\n",
            "step 6200 - loss 1.4830031394958496 - moving ave loss 1.529665395564137\n",
            "step 6201 - loss 1.3949799537658691 - moving ave loss 1.5161968513843103\n",
            "step 6202 - loss 1.180790901184082 - moving ave loss 1.4826562563642873\n",
            "step 6203 - loss 1.5901867151260376 - moving ave loss 1.4934093022404624\n",
            "step 6204 - loss 1.190744400024414 - moving ave loss 1.4631428120188577\n",
            "step 6205 - loss 1.4435654878616333 - moving ave loss 1.4611850796031352\n",
            "step 6206 - loss 1.299898386001587 - moving ave loss 1.4450564102429804\n",
            "step 6207 - loss 1.5221441984176636 - moving ave loss 1.4527651890604487\n",
            "step 6208 - loss 1.4980418682098389 - moving ave loss 1.4572928569753878\n",
            "Finish 16 epoch(es)\n",
            "step 6209 - loss 1.5885155200958252 - moving ave loss 1.4704151232874316\n",
            "step 6210 - loss 1.567711591720581 - moving ave loss 1.4801447701307466\n",
            "step 6211 - loss 1.3857595920562744 - moving ave loss 1.4707062523232994\n",
            "step 6212 - loss 1.2604024410247803 - moving ave loss 1.4496758711934474\n",
            "step 6213 - loss 1.486107587814331 - moving ave loss 1.4533190428555358\n",
            "step 6214 - loss 1.9222450256347656 - moving ave loss 1.5002116411334587\n",
            "step 6215 - loss 1.3584458827972412 - moving ave loss 1.486035065299837\n",
            "step 6216 - loss 1.592252254486084 - moving ave loss 1.4966567842184617\n",
            "step 6217 - loss 1.2971882820129395 - moving ave loss 1.4767099339979095\n",
            "step 6218 - loss 1.288409948348999 - moving ave loss 1.4578799354330185\n",
            "step 6219 - loss 1.4335010051727295 - moving ave loss 1.4554420424069898\n",
            "step 6220 - loss 1.3396817445755005 - moving ave loss 1.443866012623841\n",
            "step 6221 - loss 1.4609723091125488 - moving ave loss 1.4455766422727119\n",
            "Finish 17 epoch(es)\n",
            "step 6222 - loss 1.4524085521697998 - moving ave loss 1.4462598332624206\n",
            "step 6223 - loss 1.6993844509124756 - moving ave loss 1.4715722950274261\n",
            "step 6224 - loss 1.3485369682312012 - moving ave loss 1.4592687623478038\n",
            "step 6225 - loss 1.7580989599227905 - moving ave loss 1.4891517821053026\n",
            "step 6226 - loss 1.3719944953918457 - moving ave loss 1.477436053433957\n",
            "step 6227 - loss 1.5670112371444702 - moving ave loss 1.4863935718050085\n",
            "step 6228 - loss 1.6154588460922241 - moving ave loss 1.49930009923373\n",
            "step 6229 - loss 1.4570764303207397 - moving ave loss 1.4950777323424311\n",
            "step 6230 - loss 1.5500261783599854 - moving ave loss 1.5005725769441867\n",
            "step 6231 - loss 1.4075777530670166 - moving ave loss 1.4912730945564698\n",
            "step 6232 - loss 1.3306777477264404 - moving ave loss 1.475213559873467\n",
            "step 6233 - loss 1.4422228336334229 - moving ave loss 1.4719144872494625\n",
            "step 6234 - loss 1.6007404327392578 - moving ave loss 1.4847970817984422\n",
            "Finish 18 epoch(es)\n",
            "step 6235 - loss 1.6574676036834717 - moving ave loss 1.5020641339869452\n",
            "step 6236 - loss 1.3272087574005127 - moving ave loss 1.484578596328302\n",
            "step 6237 - loss 1.4912208318710327 - moving ave loss 1.4852428198825751\n",
            "step 6238 - loss 1.4147253036499023 - moving ave loss 1.4781910682593078\n",
            "step 6239 - loss 1.4505397081375122 - moving ave loss 1.4754259322471284\n",
            "step 6240 - loss 1.4609719514846802 - moving ave loss 1.4739805341708836\n",
            "step 6241 - loss 1.4329721927642822 - moving ave loss 1.4698797000302235\n",
            "step 6242 - loss 1.423626184463501 - moving ave loss 1.4652543484735512\n",
            "step 6243 - loss 1.5357165336608887 - moving ave loss 1.472300566992285\n",
            "step 6244 - loss 1.2521567344665527 - moving ave loss 1.4502861837397119\n",
            "step 6245 - loss 1.4660104513168335 - moving ave loss 1.451858610497424\n",
            "step 6246 - loss 1.6109082698822021 - moving ave loss 1.467763576435902\n",
            "step 6247 - loss 1.371056079864502 - moving ave loss 1.458092826778762\n",
            "Finish 19 epoch(es)\n",
            "step 6248 - loss 1.5758488178253174 - moving ave loss 1.4698684258834178\n",
            "step 6249 - loss 1.5088005065917969 - moving ave loss 1.4737616339542556\n",
            "step 6250 - loss 1.3374961614608765 - moving ave loss 1.4601350867049177\n",
            "Checkpoint at step 6250\n",
            "step 6251 - loss 1.398503065109253 - moving ave loss 1.4539718845453513\n",
            "step 6252 - loss 1.558176875114441 - moving ave loss 1.4643923836022603\n",
            "step 6253 - loss 1.5384128093719482 - moving ave loss 1.4717944261792293\n",
            "step 6254 - loss 1.2463246583938599 - moving ave loss 1.4492474494006924\n",
            "step 6255 - loss 1.721497654914856 - moving ave loss 1.4764724699521088\n",
            "step 6256 - loss 1.4032816886901855 - moving ave loss 1.4691533918259165\n",
            "step 6257 - loss 1.3510828018188477 - moving ave loss 1.4573463328252096\n",
            "step 6258 - loss 1.4284989833831787 - moving ave loss 1.4544615978810067\n",
            "step 6259 - loss 1.47294020652771 - moving ave loss 1.456309458745677\n",
            "step 6260 - loss 1.5000520944595337 - moving ave loss 1.4606837223170628\n",
            "Finish 20 epoch(es)\n",
            "step 6261 - loss 1.5227837562561035 - moving ave loss 1.4668937257109669\n",
            "step 6262 - loss 1.6584848165512085 - moving ave loss 1.486052834794991\n",
            "step 6263 - loss 1.3827743530273438 - moving ave loss 1.4757249866182263\n",
            "step 6264 - loss 1.479994773864746 - moving ave loss 1.4761519653428783\n",
            "step 6265 - loss 1.4138598442077637 - moving ave loss 1.4699227532293668\n",
            "step 6266 - loss 1.2471565008163452 - moving ave loss 1.4476461279880646\n",
            "step 6267 - loss 1.4969443082809448 - moving ave loss 1.4525759460173526\n",
            "step 6268 - loss 1.4548568725585938 - moving ave loss 1.452804038671477\n",
            "step 6269 - loss 1.3340293169021606 - moving ave loss 1.4409265664945454\n",
            "step 6270 - loss 1.4693434238433838 - moving ave loss 1.4437682522294293\n",
            "step 6271 - loss 1.3325300216674805 - moving ave loss 1.4326444291732343\n",
            "step 6272 - loss 1.6319997310638428 - moving ave loss 1.4525799593622952\n",
            "step 6273 - loss 1.2171688079833984 - moving ave loss 1.4290388442244055\n",
            "Finish 21 epoch(es)\n",
            "step 6274 - loss 1.4482882022857666 - moving ave loss 1.4309637800305415\n",
            "step 6275 - loss 1.5400664806365967 - moving ave loss 1.441874050091147\n",
            "step 6276 - loss 1.6114466190338135 - moving ave loss 1.4588313069854137\n",
            "step 6277 - loss 1.2556991577148438 - moving ave loss 1.438518092058357\n",
            "step 6278 - loss 1.5111842155456543 - moving ave loss 1.4457847044070866\n",
            "step 6279 - loss 1.3234443664550781 - moving ave loss 1.4335506706118857\n",
            "step 6280 - loss 1.3134483098983765 - moving ave loss 1.421540434540535\n",
            "step 6281 - loss 1.6137440204620361 - moving ave loss 1.4407607931326851\n",
            "step 6282 - loss 1.4448206424713135 - moving ave loss 1.441166778066548\n",
            "step 6283 - loss 1.3789416551589966 - moving ave loss 1.434944265775793\n",
            "step 6284 - loss 1.4524073600769043 - moving ave loss 1.436690575205904\n",
            "step 6285 - loss 1.3708748817443848 - moving ave loss 1.430109005859752\n",
            "step 6286 - loss 1.4223361015319824 - moving ave loss 1.4293317154269751\n",
            "Finish 22 epoch(es)\n",
            "step 6287 - loss 1.4907290935516357 - moving ave loss 1.435471453239441\n",
            "step 6288 - loss 1.1886223554611206 - moving ave loss 1.410786543461609\n",
            "step 6289 - loss 1.24853515625 - moving ave loss 1.394561404740448\n",
            "step 6290 - loss 1.4620230197906494 - moving ave loss 1.4013075662454682\n",
            "step 6291 - loss 1.620511770248413 - moving ave loss 1.4232279866457627\n",
            "step 6292 - loss 1.3172358274459839 - moving ave loss 1.4126287707257847\n",
            "step 6293 - loss 1.3362884521484375 - moving ave loss 1.4049947388680502\n",
            "step 6294 - loss 1.596330165863037 - moving ave loss 1.424128281567549\n",
            "step 6295 - loss 1.5626519918441772 - moving ave loss 1.4379806525952117\n",
            "step 6296 - loss 1.0335705280303955 - moving ave loss 1.39753964013873\n",
            "step 6297 - loss 1.3431826829910278 - moving ave loss 1.39210394442396\n",
            "step 6298 - loss 1.3784539699554443 - moving ave loss 1.3907389469771085\n",
            "step 6299 - loss 1.324177861213684 - moving ave loss 1.3840828384007662\n",
            "Finish 23 epoch(es)\n",
            "step 6300 - loss 1.6338658332824707 - moving ave loss 1.4090611378889366\n",
            "step 6301 - loss 1.352150797843933 - moving ave loss 1.4033701038844364\n",
            "step 6302 - loss 1.2167259454727173 - moving ave loss 1.3847056880432644\n",
            "step 6303 - loss 1.346071481704712 - moving ave loss 1.3808422674094092\n",
            "step 6304 - loss 1.571972370147705 - moving ave loss 1.399955277683239\n",
            "step 6305 - loss 1.4083356857299805 - moving ave loss 1.400793318487913\n",
            "step 6306 - loss 1.3189356327056885 - moving ave loss 1.3926075499096908\n",
            "step 6307 - loss 1.4836028814315796 - moving ave loss 1.4017070830618799\n",
            "step 6308 - loss 1.37288498878479 - moving ave loss 1.3988248736341709\n",
            "step 6309 - loss 1.2760885953903198 - moving ave loss 1.3865512458097857\n",
            "step 6310 - loss 1.3517152070999146 - moving ave loss 1.3830676419387986\n",
            "step 6311 - loss 1.5501667261123657 - moving ave loss 1.3997775503561554\n",
            "step 6312 - loss 1.3956289291381836 - moving ave loss 1.3993626882343582\n",
            "Finish 24 epoch(es)\n",
            "step 6313 - loss 1.2342426776885986 - moving ave loss 1.3828506871797823\n",
            "step 6314 - loss 1.1565172672271729 - moving ave loss 1.3602173451845214\n",
            "step 6315 - loss 1.314781904220581 - moving ave loss 1.3556738010881275\n",
            "step 6316 - loss 1.3549563884735107 - moving ave loss 1.355602059826666\n",
            "step 6317 - loss 1.4246561527252197 - moving ave loss 1.3625074691165213\n",
            "step 6318 - loss 1.4633734226226807 - moving ave loss 1.3725940644671373\n",
            "step 6319 - loss 1.2314503192901611 - moving ave loss 1.3584796899494398\n",
            "step 6320 - loss 1.5690523386001587 - moving ave loss 1.3795369548145118\n",
            "step 6321 - loss 1.2056550979614258 - moving ave loss 1.3621487691292034\n",
            "step 6322 - loss 1.4300445318222046 - moving ave loss 1.3689383453985036\n",
            "step 6323 - loss 1.5767799615859985 - moving ave loss 1.3897225070172532\n",
            "step 6324 - loss 1.5766657590866089 - moving ave loss 1.4084168322241888\n",
            "step 6325 - loss 1.3676071166992188 - moving ave loss 1.4043358606716918\n",
            "Finish 25 epoch(es)\n",
            "step 6326 - loss 1.4646717309951782 - moving ave loss 1.4103694477040405\n",
            "step 6327 - loss 1.1493935585021973 - moving ave loss 1.384271858783856\n",
            "step 6328 - loss 1.4705612659454346 - moving ave loss 1.3929007995000138\n",
            "step 6329 - loss 1.1897914409637451 - moving ave loss 1.372589863646387\n",
            "step 6330 - loss 1.6580543518066406 - moving ave loss 1.4011363124624123\n",
            "step 6331 - loss 1.3787791728973389 - moving ave loss 1.398900598505905\n",
            "step 6332 - loss 1.434826374053955 - moving ave loss 1.40249317606071\n",
            "step 6333 - loss 1.5070689916610718 - moving ave loss 1.412950757620746\n",
            "step 6334 - loss 1.4778521060943604 - moving ave loss 1.4194408924681075\n",
            "step 6335 - loss 1.3281883001327515 - moving ave loss 1.4103156332345719\n",
            "step 6336 - loss 1.4447319507598877 - moving ave loss 1.4137572649871035\n",
            "step 6337 - loss 1.1831248998641968 - moving ave loss 1.3906940284748128\n",
            "step 6338 - loss 1.3500028848648071 - moving ave loss 1.3866249141138123\n",
            "Finish 26 epoch(es)\n",
            "step 6339 - loss 1.3165202140808105 - moving ave loss 1.379614444110512\n",
            "step 6340 - loss 1.679499864578247 - moving ave loss 1.4096029861572856\n",
            "step 6341 - loss 1.4009718894958496 - moving ave loss 1.408739876491142\n",
            "step 6342 - loss 1.3115224838256836 - moving ave loss 1.3990181372245964\n",
            "step 6343 - loss 1.569939136505127 - moving ave loss 1.4161102371526495\n",
            "step 6344 - loss 1.4154913425445557 - moving ave loss 1.41604834769184\n",
            "step 6345 - loss 1.235713005065918 - moving ave loss 1.3980148134292476\n",
            "step 6346 - loss 1.3955082893371582 - moving ave loss 1.3977641610200386\n",
            "step 6347 - loss 1.3506207466125488 - moving ave loss 1.3930498195792897\n",
            "step 6348 - loss 1.1614117622375488 - moving ave loss 1.3698860138451159\n",
            "step 6349 - loss 1.3310656547546387 - moving ave loss 1.3660039779360682\n",
            "step 6350 - loss 1.397308349609375 - moving ave loss 1.3691344151033988\n",
            "step 6351 - loss 1.3276331424713135 - moving ave loss 1.3649842878401903\n",
            "Finish 27 epoch(es)\n",
            "step 6352 - loss 1.3339277505874634 - moving ave loss 1.3618786341149176\n",
            "step 6353 - loss 1.1645193099975586 - moving ave loss 1.3421427017031817\n",
            "step 6354 - loss 1.2880589962005615 - moving ave loss 1.3367343311529196\n",
            "step 6355 - loss 1.6667553186416626 - moving ave loss 1.3697364299017938\n",
            "step 6356 - loss 1.3083827495574951 - moving ave loss 1.363601061867364\n",
            "step 6357 - loss 1.3738868236541748 - moving ave loss 1.3646296380460452\n",
            "step 6358 - loss 1.470406413078308 - moving ave loss 1.3752073155492717\n",
            "step 6359 - loss 1.3380029201507568 - moving ave loss 1.37148687600942\n",
            "step 6360 - loss 1.4737802743911743 - moving ave loss 1.3817162158475955\n",
            "step 6361 - loss 1.1255706548690796 - moving ave loss 1.356101659749744\n",
            "step 6362 - loss 1.2163594961166382 - moving ave loss 1.3421274433864334\n",
            "step 6363 - loss 1.1237688064575195 - moving ave loss 1.320291579693542\n",
            "step 6364 - loss 1.3324196338653564 - moving ave loss 1.3215043851107233\n",
            "Finish 28 epoch(es)\n",
            "step 6365 - loss 1.012739896774292 - moving ave loss 1.2906279362770803\n",
            "step 6366 - loss 1.234729290008545 - moving ave loss 1.2850380716502268\n",
            "step 6367 - loss 1.231245756149292 - moving ave loss 1.2796588401001334\n",
            "step 6368 - loss 1.1656228303909302 - moving ave loss 1.2682552391292132\n",
            "step 6369 - loss 1.3377070426940918 - moving ave loss 1.2752004194857012\n",
            "step 6370 - loss 1.349199891090393 - moving ave loss 1.2826003666461705\n",
            "step 6371 - loss 1.1855249404907227 - moving ave loss 1.2728928240306259\n",
            "step 6372 - loss 1.6248164176940918 - moving ave loss 1.3080851833969727\n",
            "step 6373 - loss 1.456010103225708 - moving ave loss 1.3228776753798464\n",
            "step 6374 - loss 1.3757948875427246 - moving ave loss 1.3281693965961343\n",
            "step 6375 - loss 1.5938119888305664 - moving ave loss 1.3547336558195775\n",
            "Checkpoint at step 6375\n",
            "step 6376 - loss 1.3817498683929443 - moving ave loss 1.3574352770769142\n",
            "step 6377 - loss 1.163006067276001 - moving ave loss 1.3379923560968228\n",
            "Finish 29 epoch(es)\n",
            "step 6378 - loss 1.3282227516174316 - moving ave loss 1.3370153956488837\n",
            "step 6379 - loss 1.3181661367416382 - moving ave loss 1.3351304697581592\n",
            "step 6380 - loss 1.2891266345977783 - moving ave loss 1.3305300862421212\n",
            "step 6381 - loss 1.267770767211914 - moving ave loss 1.3242541543391004\n",
            "step 6382 - loss 1.2890020608901978 - moving ave loss 1.3207289449942103\n",
            "step 6383 - loss 1.4291081428527832 - moving ave loss 1.3315668647800678\n",
            "step 6384 - loss 1.3269094228744507 - moving ave loss 1.3311011205895062\n",
            "step 6385 - loss 1.5034360885620117 - moving ave loss 1.3483346173867568\n",
            "step 6386 - loss 1.6113287210464478 - moving ave loss 1.3746340277527258\n",
            "step 6387 - loss 1.2350921630859375 - moving ave loss 1.360679841286047\n",
            "step 6388 - loss 1.28416109085083 - moving ave loss 1.3530279662425253\n",
            "step 6389 - loss 1.252756118774414 - moving ave loss 1.3430007814957141\n",
            "step 6390 - loss 1.491497278213501 - moving ave loss 1.3578504311674928\n",
            "Finish 30 epoch(es)\n",
            "step 6391 - loss 1.0938746929168701 - moving ave loss 1.3314528573424305\n",
            "step 6392 - loss 1.1594375371932983 - moving ave loss 1.3142513253275172\n",
            "step 6393 - loss 1.235466718673706 - moving ave loss 1.306372864662136\n",
            "step 6394 - loss 1.4087185859680176 - moving ave loss 1.3166074367927243\n",
            "step 6395 - loss 1.2367851734161377 - moving ave loss 1.3086252104550655\n",
            "step 6396 - loss 1.3367135524749756 - moving ave loss 1.3114340446570565\n",
            "step 6397 - loss 1.2607142925262451 - moving ave loss 1.3063620694439755\n",
            "step 6398 - loss 1.5965315103530884 - moving ave loss 1.3353790135348866\n",
            "step 6399 - loss 1.343488335609436 - moving ave loss 1.3361899457423416\n",
            "step 6400 - loss 1.2237592935562134 - moving ave loss 1.3249468805237288\n",
            "step 6401 - loss 1.3449923992156982 - moving ave loss 1.326951432392926\n",
            "step 6402 - loss 1.2322014570236206 - moving ave loss 1.3174764348559953\n",
            "step 6403 - loss 1.5610772371292114 - moving ave loss 1.341836515083317\n",
            "Finish 31 epoch(es)\n",
            "step 6404 - loss 1.274845838546753 - moving ave loss 1.3351374474296605\n",
            "step 6405 - loss 1.1901271343231201 - moving ave loss 1.3206364161190065\n",
            "step 6406 - loss 1.3803229331970215 - moving ave loss 1.3266050678268082\n",
            "step 6407 - loss 1.3490664958953857 - moving ave loss 1.328851210633666\n",
            "step 6408 - loss 1.3966830968856812 - moving ave loss 1.3356343992588675\n",
            "step 6409 - loss 1.1898908615112305 - moving ave loss 1.321060045484104\n",
            "step 6410 - loss 1.0353134870529175 - moving ave loss 1.2924853896409854\n",
            "step 6411 - loss 1.2266818284988403 - moving ave loss 1.285905033526771\n",
            "step 6412 - loss 1.304194688796997 - moving ave loss 1.2877339990537937\n",
            "step 6413 - loss 1.2574763298034668 - moving ave loss 1.284708232128761\n",
            "step 6414 - loss 1.453446388244629 - moving ave loss 1.3015820477403477\n",
            "step 6415 - loss 1.473093032836914 - moving ave loss 1.3187331462500043\n",
            "step 6416 - loss 1.4934911727905273 - moving ave loss 1.3362089489040565\n",
            "Finish 32 epoch(es)\n",
            "step 6417 - loss 1.1483726501464844 - moving ave loss 1.3174253190282994\n",
            "step 6418 - loss 1.2552376985549927 - moving ave loss 1.3112065569809688\n",
            "step 6419 - loss 1.4630284309387207 - moving ave loss 1.326388744376744\n",
            "step 6420 - loss 1.1301113367080688 - moving ave loss 1.3067610036098765\n",
            "step 6421 - loss 1.053280234336853 - moving ave loss 1.2814129266825742\n",
            "step 6422 - loss 1.4351457357406616 - moving ave loss 1.296786207588383\n",
            "step 6423 - loss 1.191532850265503 - moving ave loss 1.2862608718560948\n",
            "step 6424 - loss 1.1329469680786133 - moving ave loss 1.2709294814783467\n",
            "step 6425 - loss 1.2189456224441528 - moving ave loss 1.2657310955749272\n",
            "step 6426 - loss 1.4096274375915527 - moving ave loss 1.2801207297765897\n",
            "step 6427 - loss 1.3515625 - moving ave loss 1.2872649067989308\n",
            "step 6428 - loss 1.6083797216415405 - moving ave loss 1.319376388283192\n",
            "step 6429 - loss 1.4425091743469238 - moving ave loss 1.3316896668895652\n",
            "Finish 33 epoch(es)\n",
            "step 6430 - loss 1.3330971002578735 - moving ave loss 1.3318304102263963\n",
            "step 6431 - loss 1.3112101554870605 - moving ave loss 1.3297683847524628\n",
            "step 6432 - loss 1.320742130279541 - moving ave loss 1.3288657593051707\n",
            "step 6433 - loss 1.0213693380355835 - moving ave loss 1.2981161171782118\n",
            "step 6434 - loss 1.3096733093261719 - moving ave loss 1.2992718363930078\n",
            "step 6435 - loss 1.1839138269424438 - moving ave loss 1.2877360354479515\n",
            "step 6436 - loss 1.258967399597168 - moving ave loss 1.2848591718628732\n",
            "step 6437 - loss 1.3849635124206543 - moving ave loss 1.2948696059186513\n",
            "step 6438 - loss 1.3343634605407715 - moving ave loss 1.2988189913808632\n",
            "step 6439 - loss 1.1408002376556396 - moving ave loss 1.2830171160083408\n",
            "step 6440 - loss 1.310426950454712 - moving ave loss 1.285758099452978\n",
            "step 6441 - loss 1.5136184692382812 - moving ave loss 1.3085441364315082\n",
            "step 6442 - loss 1.5355541706085205 - moving ave loss 1.3312451398492096\n",
            "Finish 34 epoch(es)\n",
            "step 6443 - loss 1.1756798028945923 - moving ave loss 1.3156886061537478\n",
            "step 6444 - loss 1.429654598236084 - moving ave loss 1.3270852053619815\n",
            "step 6445 - loss 1.6539136171340942 - moving ave loss 1.3597680465391928\n",
            "step 6446 - loss 1.469917893409729 - moving ave loss 1.3707830312262466\n",
            "step 6447 - loss 1.1405531167984009 - moving ave loss 1.3477600397834621\n",
            "step 6448 - loss 1.2730507850646973 - moving ave loss 1.3402891143115858\n",
            "step 6449 - loss 1.4150185585021973 - moving ave loss 1.347762058730647\n",
            "step 6450 - loss 1.3306511640548706 - moving ave loss 1.3460509692630693\n",
            "step 6451 - loss 1.2191190719604492 - moving ave loss 1.3333577795328073\n",
            "step 6452 - loss 1.348374366760254 - moving ave loss 1.334859438255552\n",
            "step 6453 - loss 1.287299633026123 - moving ave loss 1.3301034577326092\n",
            "step 6454 - loss 1.277506709098816 - moving ave loss 1.3248437828692299\n",
            "step 6455 - loss 1.5147812366485596 - moving ave loss 1.3438375282471628\n",
            "Finish 35 epoch(es)\n",
            "step 6456 - loss 1.4995797872543335 - moving ave loss 1.3594117541478798\n",
            "step 6457 - loss 1.1400058269500732 - moving ave loss 1.3374711614280992\n",
            "step 6458 - loss 1.2826380729675293 - moving ave loss 1.3319878525820423\n",
            "step 6459 - loss 1.4281129837036133 - moving ave loss 1.3416003656941995\n",
            "step 6460 - loss 1.3771820068359375 - moving ave loss 1.3451585298083732\n",
            "step 6461 - loss 1.1616487503051758 - moving ave loss 1.3268075518580535\n",
            "step 6462 - loss 0.9730210304260254 - moving ave loss 1.2914288997148506\n",
            "step 6463 - loss 1.4105215072631836 - moving ave loss 1.3033381604696839\n",
            "step 6464 - loss 1.379798173904419 - moving ave loss 1.3109841618131575\n",
            "step 6465 - loss 1.2583855390548706 - moving ave loss 1.3057242995373288\n",
            "step 6466 - loss 1.21107816696167 - moving ave loss 1.296259686279763\n",
            "step 6467 - loss 1.2495989799499512 - moving ave loss 1.2915936156467818\n",
            "step 6468 - loss 1.2862557172775269 - moving ave loss 1.2910598258098562\n",
            "Finish 36 epoch(es)\n",
            "step 6469 - loss 1.3527666330337524 - moving ave loss 1.297230506532246\n",
            "step 6470 - loss 1.1851222515106201 - moving ave loss 1.2860196810300835\n",
            "step 6471 - loss 1.2115384340286255 - moving ave loss 1.2785715563299376\n",
            "step 6472 - loss 1.526144027709961 - moving ave loss 1.30332880346794\n",
            "step 6473 - loss 1.0698106288909912 - moving ave loss 1.279976986010245\n",
            "step 6474 - loss 1.2836980819702148 - moving ave loss 1.280349095606242\n",
            "step 6475 - loss 1.3036718368530273 - moving ave loss 1.2826813697309205\n",
            "step 6476 - loss 1.3589117527008057 - moving ave loss 1.2903044080279091\n",
            "step 6477 - loss 1.079237937927246 - moving ave loss 1.2691977610178429\n",
            "step 6478 - loss 1.1963140964508057 - moving ave loss 1.2619093945611393\n",
            "step 6479 - loss 1.5063412189483643 - moving ave loss 1.2863525769998618\n",
            "step 6480 - loss 1.50489342212677 - moving ave loss 1.3082066615125527\n",
            "step 6481 - loss 1.164538860321045 - moving ave loss 1.293839881393402\n",
            "Finish 37 epoch(es)\n",
            "step 6482 - loss 1.0597772598266602 - moving ave loss 1.2704336192367278\n",
            "step 6483 - loss 1.4216943979263306 - moving ave loss 1.285559697105688\n",
            "step 6484 - loss 1.2205480337142944 - moving ave loss 1.2790585307665487\n",
            "step 6485 - loss 1.3490251302719116 - moving ave loss 1.2860551907170852\n",
            "step 6486 - loss 1.200166940689087 - moving ave loss 1.2774663657142853\n",
            "step 6487 - loss 1.2227543592453003 - moving ave loss 1.2719951650673866\n",
            "step 6488 - loss 1.379328727722168 - moving ave loss 1.282728521332865\n",
            "step 6489 - loss 1.4816217422485352 - moving ave loss 1.302617843424432\n",
            "step 6490 - loss 1.4099647998809814 - moving ave loss 1.3133525390700869\n",
            "step 6491 - loss 1.266438364982605 - moving ave loss 1.3086611216613386\n",
            "step 6492 - loss 1.3881871700286865 - moving ave loss 1.3166137264980735\n",
            "step 6493 - loss 1.1148008108139038 - moving ave loss 1.2964324349296565\n",
            "step 6494 - loss 1.0319206714630127 - moving ave loss 1.2699812585829924\n",
            "Finish 38 epoch(es)\n",
            "step 6495 - loss 1.61653733253479 - moving ave loss 1.3046368659781722\n",
            "step 6496 - loss 1.6395891904830933 - moving ave loss 1.3381320984286642\n",
            "step 6497 - loss 1.1923091411590576 - moving ave loss 1.3235498027017036\n",
            "step 6498 - loss 1.3911149501800537 - moving ave loss 1.3303063174495389\n",
            "step 6499 - loss 1.167757511138916 - moving ave loss 1.3140514368184766\n",
            "step 6500 - loss 1.2561665773391724 - moving ave loss 1.3082629508705463\n",
            "Checkpoint at step 6500\n",
            "step 6501 - loss 1.2865899801254272 - moving ave loss 1.3060956537960344\n",
            "step 6502 - loss 0.978989839553833 - moving ave loss 1.2733850723718143\n",
            "step 6503 - loss 1.1205366849899292 - moving ave loss 1.258100233633626\n",
            "step 6504 - loss 1.3282785415649414 - moving ave loss 1.2651180644267577\n",
            "step 6505 - loss 1.4930161237716675 - moving ave loss 1.2879078703612485\n",
            "step 6506 - loss 1.0625123977661133 - moving ave loss 1.265368323101735\n",
            "step 6507 - loss 1.1747149229049683 - moving ave loss 1.2563029830820585\n",
            "Finish 39 epoch(es)\n",
            "step 6508 - loss 1.332429051399231 - moving ave loss 1.2639155899137757\n",
            "step 6509 - loss 1.5174341201782227 - moving ave loss 1.2892674429402204\n",
            "step 6510 - loss 1.0991435050964355 - moving ave loss 1.270255049155842\n",
            "step 6511 - loss 1.2996282577514648 - moving ave loss 1.2731923700154042\n",
            "step 6512 - loss 1.0615613460540771 - moving ave loss 1.2520292676192715\n",
            "step 6513 - loss 1.2374553680419922 - moving ave loss 1.2505718776615438\n",
            "step 6514 - loss 0.9654031991958618 - moving ave loss 1.2220550098149756\n",
            "step 6515 - loss 1.195674180984497 - moving ave loss 1.2194169269319277\n",
            "step 6516 - loss 1.6602834463119507 - moving ave loss 1.2635035788699303\n",
            "step 6517 - loss 1.1319314241409302 - moving ave loss 1.2503463633970304\n",
            "step 6518 - loss 1.2826833724975586 - moving ave loss 1.2535800643070834\n",
            "step 6519 - loss 1.0228261947631836 - moving ave loss 1.2305046773526935\n",
            "step 6520 - loss 1.4263091087341309 - moving ave loss 1.2500851204908374\n",
            "Finish 40 epoch(es)\n",
            "step 6521 - loss 1.480001449584961 - moving ave loss 1.2730767534002496\n",
            "step 6522 - loss 1.1171034574508667 - moving ave loss 1.2574794238053115\n",
            "step 6523 - loss 1.1245026588439941 - moving ave loss 1.2441817473091796\n",
            "step 6524 - loss 1.443394422531128 - moving ave loss 1.2641030148313745\n",
            "step 6525 - loss 1.3485937118530273 - moving ave loss 1.2725520845335399\n",
            "step 6526 - loss 1.0873119831085205 - moving ave loss 1.254028074391038\n",
            "step 6527 - loss 1.2251052856445312 - moving ave loss 1.2511357955163873\n",
            "step 6528 - loss 1.327202320098877 - moving ave loss 1.2587424479746363\n",
            "step 6529 - loss 1.1992480754852295 - moving ave loss 1.2527930107256955\n",
            "step 6530 - loss 1.2906343936920166 - moving ave loss 1.2565771490223276\n",
            "step 6531 - loss 1.1765785217285156 - moving ave loss 1.2485772862929465\n",
            "step 6532 - loss 1.1260766983032227 - moving ave loss 1.236327227493974\n",
            "step 6533 - loss 1.035333514213562 - moving ave loss 1.2162278561659328\n",
            "Finish 41 epoch(es)\n",
            "step 6534 - loss 1.692573070526123 - moving ave loss 1.263862377601952\n",
            "step 6535 - loss 1.170093059539795 - moving ave loss 1.2544854457957362\n",
            "step 6536 - loss 1.1268105506896973 - moving ave loss 1.2417179562851322\n",
            "step 6537 - loss 1.3626859188079834 - moving ave loss 1.2538147525374175\n",
            "step 6538 - loss 1.3927381038665771 - moving ave loss 1.2677070876703336\n",
            "step 6539 - loss 1.1112781763076782 - moving ave loss 1.252064196534068\n",
            "step 6540 - loss 1.1721290349960327 - moving ave loss 1.2440706803802644\n",
            "step 6541 - loss 1.3768317699432373 - moving ave loss 1.2573467893365615\n",
            "step 6542 - loss 0.9716118574142456 - moving ave loss 1.2287732961443298\n",
            "step 6543 - loss 1.2736097574234009 - moving ave loss 1.233256942272237\n",
            "step 6544 - loss 1.372624158859253 - moving ave loss 1.2471936639309387\n",
            "step 6545 - loss 1.3567416667938232 - moving ave loss 1.2581484642172271\n",
            "step 6546 - loss 1.0694093704223633 - moving ave loss 1.2392745548377408\n",
            "Finish 42 epoch(es)\n",
            "step 6547 - loss 1.3127506971359253 - moving ave loss 1.2466221690675592\n",
            "step 6548 - loss 1.3630530834197998 - moving ave loss 1.2582652605027833\n",
            "step 6549 - loss 1.2302461862564087 - moving ave loss 1.255463353078146\n",
            "step 6550 - loss 0.9694650769233704 - moving ave loss 1.2268635254626685\n",
            "step 6551 - loss 1.3092355728149414 - moving ave loss 1.2351007301978958\n",
            "step 6552 - loss 1.25373375415802 - moving ave loss 1.2369640325939082\n",
            "step 6553 - loss 1.4760113954544067 - moving ave loss 1.260868768879958\n",
            "step 6554 - loss 1.1268091201782227 - moving ave loss 1.2474628040097846\n",
            "step 6555 - loss 1.0653105974197388 - moving ave loss 1.22924758335078\n",
            "step 6556 - loss 1.1933993101119995 - moving ave loss 1.2256627560269022\n",
            "step 6557 - loss 1.3615167140960693 - moving ave loss 1.239248151833819\n",
            "step 6558 - loss 1.498916506767273 - moving ave loss 1.2652149873271645\n",
            "step 6559 - loss 1.601453185081482 - moving ave loss 1.2988388071025965\n",
            "Finish 43 epoch(es)\n",
            "step 6560 - loss 1.0926713943481445 - moving ave loss 1.2782220658271513\n",
            "step 6561 - loss 1.261623740196228 - moving ave loss 1.276562233264059\n",
            "step 6562 - loss 1.4702190160751343 - moving ave loss 1.2959279115451665\n",
            "step 6563 - loss 1.4526714086532593 - moving ave loss 1.3116022612559757\n",
            "step 6564 - loss 1.1228506565093994 - moving ave loss 1.2927271007813181\n",
            "step 6565 - loss 1.1125787496566772 - moving ave loss 1.274712265668854\n",
            "step 6566 - loss 1.1080965995788574 - moving ave loss 1.2580506990598546\n",
            "step 6567 - loss 1.2120486497879028 - moving ave loss 1.2534504941326594\n",
            "step 6568 - loss 1.1744388341903687 - moving ave loss 1.2455493281384302\n",
            "step 6569 - loss 1.5643150806427002 - moving ave loss 1.2774259033888573\n",
            "step 6570 - loss 1.2529077529907227 - moving ave loss 1.2749740883490437\n",
            "step 6571 - loss 1.3348498344421387 - moving ave loss 1.2809616629583531\n",
            "step 6572 - loss 1.0810989141464233 - moving ave loss 1.2609753880771601\n",
            "Finish 44 epoch(es)\n",
            "step 6573 - loss 1.150231957435608 - moving ave loss 1.249901045013005\n",
            "step 6574 - loss 1.3067631721496582 - moving ave loss 1.2555872577266705\n",
            "step 6575 - loss 1.3641958236694336 - moving ave loss 1.266448114320947\n",
            "step 6576 - loss 1.2152241468429565 - moving ave loss 1.261325717573148\n",
            "step 6577 - loss 1.2957067489624023 - moving ave loss 1.2647638207120735\n",
            "step 6578 - loss 1.497105360031128 - moving ave loss 1.287997974643979\n",
            "step 6579 - loss 0.9872987270355225 - moving ave loss 1.2579280498831331\n",
            "step 6580 - loss 1.2386760711669922 - moving ave loss 1.2560028520115192\n",
            "step 6581 - loss 1.3821207284927368 - moving ave loss 1.268614639659641\n",
            "step 6582 - loss 1.260306477546692 - moving ave loss 1.267783823448346\n",
            "step 6583 - loss 1.4685178995132446 - moving ave loss 1.287857231054836\n",
            "step 6584 - loss 1.4547932147979736 - moving ave loss 1.3045508294291497\n",
            "step 6585 - loss 1.315744161605835 - moving ave loss 1.3056701626468181\n",
            "Finish 45 epoch(es)\n",
            "step 6586 - loss 1.2424336671829224 - moving ave loss 1.2993465131004287\n",
            "step 6587 - loss 1.4897834062576294 - moving ave loss 1.318390202416149\n",
            "step 6588 - loss 1.3163161277770996 - moving ave loss 1.318182794952244\n",
            "step 6589 - loss 1.0921931266784668 - moving ave loss 1.2955838281248664\n",
            "step 6590 - loss 1.4852769374847412 - moving ave loss 1.3145531390608538\n",
            "step 6591 - loss 1.2087745666503906 - moving ave loss 1.3039752818198074\n",
            "step 6592 - loss 1.0672581195831299 - moving ave loss 1.2803035655961397\n",
            "step 6593 - loss 1.2344387769699097 - moving ave loss 1.2757170867335168\n",
            "step 6594 - loss 1.2985042333602905 - moving ave loss 1.2779958013961943\n",
            "step 6595 - loss 1.2719502449035645 - moving ave loss 1.2773912457469312\n",
            "step 6596 - loss 1.1868953704833984 - moving ave loss 1.268341658220578\n",
            "step 6597 - loss 1.1182737350463867 - moving ave loss 1.253334865903159\n",
            "step 6598 - loss 1.2104394435882568 - moving ave loss 1.2490453236716685\n",
            "Finish 46 epoch(es)\n",
            "step 6599 - loss 1.107940673828125 - moving ave loss 1.2349348586873141\n",
            "step 6600 - loss 1.1334338188171387 - moving ave loss 1.2247847547002966\n",
            "step 6601 - loss 1.0127778053283691 - moving ave loss 1.2035840597631038\n",
            "step 6602 - loss 1.1391825675964355 - moving ave loss 1.197143910546437\n",
            "step 6603 - loss 1.3595868349075317 - moving ave loss 1.2133882029825465\n",
            "step 6604 - loss 1.2018511295318604 - moving ave loss 1.212234495637478\n",
            "step 6605 - loss 1.094310998916626 - moving ave loss 1.2004421459653927\n",
            "step 6606 - loss 1.3347922563552856 - moving ave loss 1.213877157004382\n",
            "step 6607 - loss 1.384608507156372 - moving ave loss 1.2309502920195812\n",
            "step 6608 - loss 1.343555212020874 - moving ave loss 1.2422107840197105\n",
            "step 6609 - loss 1.1781190633773804 - moving ave loss 1.2358016119554776\n",
            "step 6610 - loss 1.111788272857666 - moving ave loss 1.2234002780456963\n",
            "step 6611 - loss 1.4360429048538208 - moving ave loss 1.2446645407265087\n",
            "Finish 47 epoch(es)\n",
            "step 6612 - loss 1.1480655670166016 - moving ave loss 1.235004643355518\n",
            "step 6613 - loss 1.1282947063446045 - moving ave loss 1.2243336496544266\n",
            "step 6614 - loss 1.1323808431625366 - moving ave loss 1.2151383690052378\n",
            "step 6615 - loss 1.3487141132354736 - moving ave loss 1.2284959434282614\n",
            "step 6616 - loss 1.0607361793518066 - moving ave loss 1.211719967020616\n",
            "step 6617 - loss 1.0136003494262695 - moving ave loss 1.1919080052611815\n",
            "step 6618 - loss 1.5398621559143066 - moving ave loss 1.226703420326494\n",
            "step 6619 - loss 1.358799934387207 - moving ave loss 1.2399130717325653\n",
            "step 6620 - loss 1.1630401611328125 - moving ave loss 1.23222578067259\n",
            "step 6621 - loss 1.165670394897461 - moving ave loss 1.2255702420950771\n",
            "step 6622 - loss 1.2681506872177124 - moving ave loss 1.2298282866073407\n",
            "step 6623 - loss 0.9571056365966797 - moving ave loss 1.2025560216062747\n",
            "step 6624 - loss 0.9429068565368652 - moving ave loss 1.1765911050993338\n",
            "Finish 48 epoch(es)\n",
            "step 6625 - loss 1.0505235195159912 - moving ave loss 1.1639843465409996\n",
            "Checkpoint at step 6625\n",
            "step 6626 - loss 1.2937489748001099 - moving ave loss 1.1769608093669108\n",
            "step 6627 - loss 1.2078070640563965 - moving ave loss 1.1800454348358593\n",
            "step 6628 - loss 1.021102786064148 - moving ave loss 1.1641511699586882\n",
            "step 6629 - loss 1.1719154119491577 - moving ave loss 1.1649275941577353\n",
            "step 6630 - loss 1.3863943815231323 - moving ave loss 1.1870742728942751\n",
            "step 6631 - loss 1.1013009548187256 - moving ave loss 1.17849694108672\n",
            "step 6632 - loss 1.3812954425811768 - moving ave loss 1.1987767912361658\n",
            "step 6633 - loss 1.3594398498535156 - moving ave loss 1.214843097097901\n",
            "step 6634 - loss 1.3250454664230347 - moving ave loss 1.2258633340304144\n",
            "step 6635 - loss 1.021284818649292 - moving ave loss 1.2054054824923022\n",
            "step 6636 - loss 1.0295679569244385 - moving ave loss 1.1878217299355158\n",
            "step 6637 - loss 1.4027087688446045 - moving ave loss 1.2093104338264247\n",
            "Finish 49 epoch(es)\n",
            "step 6638 - loss 1.3573583364486694 - moving ave loss 1.224115224088649\n",
            "step 6639 - loss 1.2815232276916504 - moving ave loss 1.2298560244489491\n",
            "step 6640 - loss 1.2605000734329224 - moving ave loss 1.2329204293473464\n",
            "step 6641 - loss 1.0238194465637207 - moving ave loss 1.2120103310689838\n",
            "step 6642 - loss 1.0835394859313965 - moving ave loss 1.199163246555225\n",
            "step 6643 - loss 1.4911952018737793 - moving ave loss 1.2283664420870806\n",
            "step 6644 - loss 1.1643223762512207 - moving ave loss 1.2219620355034946\n",
            "step 6645 - loss 1.4125521183013916 - moving ave loss 1.2410210437832843\n",
            "step 6646 - loss 1.254279375076294 - moving ave loss 1.2423468769125854\n",
            "step 6647 - loss 1.3077921867370605 - moving ave loss 1.2488914078950328\n",
            "step 6648 - loss 1.0828609466552734 - moving ave loss 1.232288361771057\n",
            "step 6649 - loss 1.3692880868911743 - moving ave loss 1.2459883342830687\n",
            "step 6650 - loss 1.0940455198287964 - moving ave loss 1.2307940528376415\n",
            "Finish 50 epoch(es)\n",
            "step 6651 - loss 0.8550336956977844 - moving ave loss 1.193218017123656\n",
            "step 6652 - loss 1.1857218742370605 - moving ave loss 1.1924684028349963\n",
            "step 6653 - loss 1.3737890720367432 - moving ave loss 1.210600469755171\n",
            "step 6654 - loss 1.5083162784576416 - moving ave loss 1.2403720506254183\n",
            "step 6655 - loss 1.1641461849212646 - moving ave loss 1.232749464055003\n",
            "step 6656 - loss 1.119159460067749 - moving ave loss 1.2213904636562776\n",
            "step 6657 - loss 1.459742546081543 - moving ave loss 1.2452256718988042\n",
            "step 6658 - loss 1.1716492176055908 - moving ave loss 1.2378680264694828\n",
            "step 6659 - loss 1.1387981176376343 - moving ave loss 1.227961035586298\n",
            "step 6660 - loss 1.231673002243042 - moving ave loss 1.2283322322519723\n",
            "step 6661 - loss 0.915392279624939 - moving ave loss 1.197038236989269\n",
            "step 6662 - loss 0.8516414165496826 - moving ave loss 1.1624985549453102\n",
            "step 6663 - loss 1.3871479034423828 - moving ave loss 1.1849634897950174\n",
            "Finish 51 epoch(es)\n",
            "step 6664 - loss 1.4174115657806396 - moving ave loss 1.2082082973935797\n",
            "step 6665 - loss 1.3315452337265015 - moving ave loss 1.2205419910268718\n",
            "step 6666 - loss 1.4521520137786865 - moving ave loss 1.2437029933020534\n",
            "step 6667 - loss 1.3279074430465698 - moving ave loss 1.252123438276505\n",
            "step 6668 - loss 0.9598035216331482 - moving ave loss 1.2228914466121694\n",
            "step 6669 - loss 1.362243413925171 - moving ave loss 1.2368266433434696\n",
            "step 6670 - loss 1.1210854053497314 - moving ave loss 1.2252525195440958\n",
            "step 6671 - loss 1.2271521091461182 - moving ave loss 1.225442478504298\n",
            "step 6672 - loss 1.0890130996704102 - moving ave loss 1.2117995406209094\n",
            "step 6673 - loss 1.0278669595718384 - moving ave loss 1.1934062825160026\n",
            "step 6674 - loss 1.064234733581543 - moving ave loss 1.1804891276225566\n",
            "step 6675 - loss 1.1259599924087524 - moving ave loss 1.175036214101176\n",
            "step 6676 - loss 1.4471238851547241 - moving ave loss 1.202244981206531\n",
            "Finish 52 epoch(es)\n",
            "step 6677 - loss 1.746514916419983 - moving ave loss 1.2566719747278763\n",
            "step 6678 - loss 1.1635429859161377 - moving ave loss 1.2473590758467024\n",
            "step 6679 - loss 0.9914530515670776 - moving ave loss 1.22176847341874\n",
            "step 6680 - loss 1.567295789718628 - moving ave loss 1.2563212050487287\n",
            "step 6681 - loss 1.4084467887878418 - moving ave loss 1.27153376342264\n",
            "step 6682 - loss 1.2461340427398682 - moving ave loss 1.268993791354363\n",
            "step 6683 - loss 1.0856719017028809 - moving ave loss 1.2506616023892148\n",
            "step 6684 - loss 1.092531681060791 - moving ave loss 1.2348486102563725\n",
            "step 6685 - loss 1.081048607826233 - moving ave loss 1.2194686100133587\n",
            "Traceback (most recent call last):\n",
            "  File \"flow\", line 6, in <module>\n",
            "    cliHandler(sys.argv)\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/cli.py\", line 33, in cliHandler\n",
            "    print('Enter training ...'); tfnet.train()\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/net/flow.py\", line 39, in train\n",
            "    for i, (x_batch, datum) in enumerate(batches):\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/net/yolo/data.py\", line 114, in shuffle\n",
            "    inp, new_feed = self._batch(train_instance)\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/net/yolov2/data.py\", line 28, in _batch\n",
            "    img = self.preprocess(path, allobj)\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/net/yolo/predict.py\", line 62, in preprocess\n",
            "    result = imcv2_affine_trans(im)\n",
            "  File \"/content/drive/My Drive/YoloV2/darkflow-master/darkflow/utils/im_transform.py\", line 20, in imcv2_affine_trans\n",
            "    h, w, c = im.shape\n",
            "AttributeError: 'NoneType' object has no attribute 'shape'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}